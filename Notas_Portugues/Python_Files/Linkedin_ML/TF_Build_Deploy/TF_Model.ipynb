{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc774593",
   "metadata": {},
   "source": [
    "# Building and Deploying Deep Learning Applications with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b8c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "tf1.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0c2f84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1)\n",
      "(400, 9)\n",
      "Note: Y values were scaled by multiplying by 0.0000036968 and adding -0.1159\n"
     ]
    }
   ],
   "source": [
    "# Load Training data set from CSV file\n",
    "training_data_df = pd.read_csv(\"./ex_files/03/sales_data_training.csv\", dtype=float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "x_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df = pd.read_csv(\"./ex_files/03/sales_data_test.csv\")\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "x_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs\n",
    "x_scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training input and outputs\n",
    "x_scaled_training = x_scaler.fit_transform(x_training)\n",
    "y_scaled_training = y_scaler.fit_transform(y_training)\n",
    "\n",
    "# It's important that the training and test data are scaled with the same scaler\n",
    "x_scaled_testing = x_scaler.transform(x_testing) \n",
    "y_scaled_testing = y_scaler.transform(y_testing)\n",
    "\n",
    "print(y_scaled_testing.shape)\n",
    "print(x_scaled_testing.shape)\n",
    "\n",
    "print(\"Note: Y values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(y_scaler.scale_[0],\n",
    "     y_scaler.min_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3fff4",
   "metadata": {},
   "source": [
    "## Define the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb743770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 22:52:13.167824: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-07 22:52:13.168490: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-07 22:52:13.168702: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lucas-Inspiron-5566): /proc/driver/nvidia/version does not exist\n",
      "2022-04-07 22:52:13.171002: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1136593 0.12273928\n",
      "5 0.1136593 0.12273928\n",
      "10 0.1136593 0.12273928\n",
      "15 0.1136593 0.12273928\n",
      "20 0.1136593 0.12273928\n",
      "25 0.1136593 0.12273928\n",
      "30 0.1136593 0.12273928\n",
      "35 0.1136593 0.12273928\n",
      "40 0.1136593 0.12273928\n",
      "45 0.1136593 0.12273928\n",
      "50 0.1136593 0.12273928\n",
      "55 0.1136593 0.12273928\n",
      "60 0.1136593 0.12273928\n",
      "65 0.1136593 0.12273928\n",
      "70 0.1136593 0.12273928\n",
      "75 0.1136593 0.12273928\n",
      "80 0.1136593 0.12273928\n",
      "85 0.1136593 0.12273928\n",
      "90 0.1136593 0.12273928\n",
      "95 0.1136593 0.12273928\n",
      "Training is complete!\n",
      "Final Training cost: 0.11365929991006851\n",
      "Final Testing cost: 0.122739277780056\n",
      "Model saved: logs/trained_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many inputs and outputs are in our neural networks\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Selection One: Define the layers of the neural network itself\n",
    "\n",
    "\n",
    "tf1.reset_default_graph()\n",
    "# Input layer\n",
    "with tf1.variable_scope('input'):\n",
    "    x = tf1.placeholder(tf.float32, shape=(None, number_of_inputs))\n",
    "    \n",
    "# Layer 1\n",
    "with tf1.variable_scope('layer_1'):\n",
    "    weights = tf1.get_variable(name='weigths1', shape=[number_of_inputs, layer_1_nodes],\n",
    "                             initializer=tf.initializers.GlorotUniform())\n",
    "    biases = tf1.get_variable(name='biases1', shape=[layer_1_nodes],\n",
    "                            initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(x, weights) + biases)\n",
    "    \n",
    "    \n",
    "# Layer 2\n",
    "with tf1.variable_scope('layer_2'):\n",
    "    weights = tf1.get_variable(name='weights2', shape=[layer_1_nodes, layer_2_nodes])\n",
    "    biases = tf1.get_variable(name='biases2', shape=[layer_2_nodes],\n",
    "                             initializer=tf.initializers.GlorotUniform())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n",
    "    \n",
    "    \n",
    "# Layer 3\n",
    "with tf1.variable_scope('layer_3'):\n",
    "    weights = tf1.get_variable(name='weights3', shape=[layer_2_nodes, layer_3_nodes])\n",
    "    biases = tf1.get_variable(name='biases3', shape=[layer_3_nodes],\n",
    "                             initializer=tf.initializers.GlorotUniform())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n",
    "    \n",
    "    \n",
    "# Output layer\n",
    "with tf1.variable_scope('output'):\n",
    "    weights = tf1.get_variable(name='weights4', shape=[layer_3_nodes, number_of_outputs])\n",
    "    biases = tf1.get_variable(name='biases4', shape=[number_of_outputs],\n",
    "                            initializer=tf.initializers.GlorotUniform())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output, weights) + biases)\n",
    "    \n",
    "# Section two: Define the cost function of the neural network that will measure the predict\n",
    "with tf1.variable_scope('cost'):\n",
    "    y = tf1.placeholder(tf.float32, shape=(None,1))\n",
    "    cost = tf.reduce_mean(tf.compat.v1.squared_difference(prediction, y))\n",
    "    \n",
    "# Section three: define the optimize function that will be run to optimize the neural network\n",
    "\n",
    "with tf1.variable_scope('train'):\n",
    "    optimizer = tf1.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    \n",
    "# Create a summary operation to log the progress of the network\n",
    "with tf1.variable_scope('logging'):\n",
    "    tf1.summary.scalar('current_cost', cost)\n",
    "    summary = tf1.summary.merge_all()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "saver = tf1.train.Saver()\n",
    "    \n",
    "    \n",
    "# Initialize a session so that we can run TensorFlow operations\n",
    "with tf1.Session() as session:\n",
    "    \n",
    "    # Run the global variable initializer to initialize all variables and layers\n",
    "    # of the graph\n",
    "    session.run(tf1.global_variables_initializer())\n",
    "    \n",
    "    # Instead load them from disk\n",
    "#     saver.restore(session, \"logs/trained_model.ckpt\")\n",
    "    \n",
    "    \n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf1.summary.FileWriter('./logs/training', session.graph)\n",
    "    testing_writer = tf1.summary.FileWriter('./logs/testing', session.graph)\n",
    "    \n",
    "    \n",
    "    # Run the optimizer over and over to train the network\n",
    "    # One epoch is one full run through the training data set\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={x: x_scaled_training, y: y_scaled_training})\n",
    "        \n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch % 5 == 0:\n",
    "            training_cost, training_summary = session.run([cost, summary], \n",
    "                                        feed_dict={x: x_scaled_training, \n",
    "                                                  y: y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary],\n",
    "                                                        feed_dict={x: x_scaled_testing,\n",
    "                                                       y: y_scaled_testing})\n",
    "            \n",
    "            # Write the current training status to the log files\n",
    "            training_writer.add_summary(training_summary, epoch)\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            \n",
    "            \n",
    "            print(epoch, training_cost, testing_cost)\n",
    "        \n",
    "    # Training is now complete!\n",
    "    print(\"Training is complete!\")\n",
    "\n",
    "\n",
    "    final_training_cost = session.run(cost, feed_dict={x: x_scaled_training, \n",
    "                                          y: y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={x: x_scaled_testing,\n",
    "                                       y: y_scaled_testing})\n",
    "\n",
    "    print(\"Final Training cost: {}\".format(final_training_cost))\n",
    "    print(\"Final Testing cost: {}\".format(final_testing_cost))\n",
    "    \n",
    "    \n",
    "    \n",
    "    save_path = saver.save(session, \"logs/trained_model.ckpt\")\n",
    "    print(\"Model saved: {}\".format(save_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
